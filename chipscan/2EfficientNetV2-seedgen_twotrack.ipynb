{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AlexNet', 'DenseNet', 'EfficientNet', 'GoogLeNet', 'GoogLeNetOutputs', 'Inception3', 'InceptionOutputs', 'MNASNet', 'MobileNetV2', 'MobileNetV3', 'RegNet', 'ResNet', 'ShuffleNetV2', 'SqueezeNet', 'VGG', '_GoogLeNetOutputs', '_InceptionOutputs', 'alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'regnet_x_16gf', 'regnet_x_1_6gf', 'regnet_x_32gf', 'regnet_x_3_2gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_8gf', 'regnet_y_16gf', 'regnet_y_1_6gf', 'regnet_y_32gf', 'regnet_y_3_2gf', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_8gf', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "\n",
    "from torchvision import models\n",
    "model_names = [name for name in dir(models) if callable(getattr(models, name))]\n",
    "print(model_names)\n",
    "# 사전 학습된 모델 로드\n",
    "\n",
    "OUTFILE1 = './seed4/ft_efficientnet_v2_m_submit_seed_'\n",
    "OUTFILE2 = '.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [2:28:01<00:00, 17.76s/it]\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:04<00:00,  1.42it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:02<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,1,1,1,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,1,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,1,0,0,1,0,1,1,1,0,0,0,0,1,0,1,1,1,1,1,1,1,0,0,0,0,1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [2:28:40<00:00, 17.84s/it]\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:04<00:00,  1.49it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:02<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,0,1,1,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,0,1,0,1,0,0,0,1,0,0,1,0,1,1,1,1,1,0,0,0,0,1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [2:30:04<00:00, 18.01s/it]\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:04<00:00,  1.46it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:02<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,1,1,1,1,0,0,0,0,1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [2:28:24<00:00, 17.81s/it]\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:05<00:00,  1.35it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:02<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,1,0,0,1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,1,1,1,0,0,0,0,1,0,1,1,1,1,1,1,1,0,0,0,0,1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [2:30:24<00:00, 18.05s/it]\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:05<00:00,  1.23it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:02<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,1,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,1,0,0,1,0,1,0,1,0,0,0,0,1,0,1,0,1,1,1,1,1,0,0,0,0,1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [2:41:53<00:00, 19.43s/it]\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:04<00:00,  1.58it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:02<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,1,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,1,0,1,0,0,0,0,1,0,1,0,0,1,1,1,1,0,0,0,0,1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [2:26:07<00:00, 17.53s/it]\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:04<00:00,  1.54it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:02<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,1,1,1,1,1,1,0,1,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,0,1,1,1,0,0,0,1,1,0,1,1,1,1,1,1,1,0,0,0,0,1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "for seed in range(73,80):\n",
    "    seed_everything(seed) # Seed 고정\n",
    "    model = models.efficientnet_b7(pretrained=True)\n",
    "    # 데이터 로딩 클래스 정의\n",
    "    class TrainDataset(Dataset):\n",
    "        def __init__(self, csv_file, transform=None):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                csv_file (string): csv 파일의 경로.\n",
    "                transform (callable, optional): 샘플에 적용될 Optional transform.\n",
    "            \"\"\"\n",
    "            self.df = pd.read_csv(csv_file)\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_path = self.df['img_path'].iloc[idx]\n",
    "            image = Image.open(img_path)\n",
    "            label = self.df.iloc[idx, 2]  # 라벨\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image,label\n",
    "\n",
    "\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, csv_file, transform=None):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                csv_file (string): csv 파일의 경로.\n",
    "                transform (callable, optional): 샘플에 적용될 Optional transform.\n",
    "            \"\"\"\n",
    "            self.df = pd.read_csv(csv_file)\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_path = self.df['img_path'].iloc[idx]\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image\n",
    "\n",
    "    # 이미지 전처리 및 임베딩\n",
    "    transform = transforms.Compose([\n",
    "        #transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "\n",
    "    train_data = TrainDataset(csv_file='./bigdata/train.csv', transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size=4, shuffle=False)\n",
    "\n",
    "    # 출력 레이어 수정\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "\n",
    "    # 이진 분류를 위한 출력 크기로 변경\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(num_ftrs, 2)\n",
    "    )\n",
    "\n",
    "    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    model.train()  # 학습 모드 설정\n",
    "    for epoch in tqdm(range(500)):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(), './bigdata/rsn_model/efnet_tt_model.pth')     \n",
    "    model.load_state_dict(torch.load('./bigdata/rsn_model/efnet_tt_model.pth'))\n",
    "    model.classifier  = nn.Identity()  # 마지막 레이어를 Identity로 설정하여 임베딩 출력\n",
    "\n",
    "\n",
    "\n",
    "    model.eval()  # 추론 모드로 설정\n",
    "\n",
    "    # 특성 추출을 위한 모델의 마지막 레이어 수정\n",
    "    #model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # 이미지를 임베딩 벡터로 변환\n",
    "    def get_embeddings(dataloader, model):\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for images in tqdm(dataloader):\n",
    "                images = images.to(device)\n",
    "                emb = model(images)\n",
    "                embeddings.append(emb.cpu().numpy().squeeze())\n",
    "        return np.concatenate(embeddings, axis=0)\n",
    "    train_data = CustomDataset(csv_file='./bigdata/train.csv', transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
    "    train_embeddings = get_embeddings(train_loader, model)\n",
    "\n",
    "\n",
    "    # Isolation Forest 모델 학습\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    clf.fit(train_embeddings)\n",
    "\n",
    "\n",
    "    # 테스트 데이터에 대해 이상 탐지 수행\n",
    "    test_data = CustomDataset(csv_file='./bigdata/test.csv', transform=transform)\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "    test_embeddings = get_embeddings(test_loader, model)\n",
    "    test_pred = clf.predict(test_embeddings)\n",
    "\n",
    "    # Isolation Forest의 예측 결과(이상 = -1, 정상 = 1)를 이상 = 1, 정상 = 0으로 변환\n",
    "    test_pred = np.where(test_pred == -1, 1, 0)\n",
    "\n",
    "\n",
    "    submit = pd.read_csv('./bigdata/sample_submission.csv')\n",
    "    submit['label'] = test_pred\n",
    "    for i in test_pred:\n",
    "        print(i,end=\",\")\n",
    "\n",
    "\n",
    "    submit.to_csv(OUTFILE1+str(seed)+OUTFILE2, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpvenv",
   "language": "python",
   "name": "vpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
