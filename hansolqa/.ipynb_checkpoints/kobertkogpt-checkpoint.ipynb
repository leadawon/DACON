{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3' # nvidia-smi로 비어있는 gpu 확인하고 여기서 선택할것!\n",
    "\n",
    "\n",
    "#from huggingface_hub import notebook_login\n",
    "\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerFast as BaseGPT2Tokenizer,\n",
    "    EncoderDecoderModel,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    AdamW\n",
    ")\n",
    "import pickle\n",
    "\n",
    "from asset.tokenization_kobert import KoBertTokenizer\n",
    "from asset import tokenization_kobert\n",
    "src_tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Tokenizer(BaseGPT2Tokenizer):\n",
    "    def build_inputs_with_special_tokens(self, token_ids, _):\n",
    "        return token_ids + [self.eos_token_id]\n",
    "trg_tokenizer = GPT2Tokenizer.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_dict(input_dict, ratio = 0.01, seed = 42):\n",
    "    split_point = int(len(input_dict['utterance']) * ratio)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(input_dict['utterance'])\n",
    "    valid_dict = deepcopy(input_dict)\n",
    "    train_dict = input_dict\n",
    "\n",
    "    valid_dict['utterance'] = input_dict['utterance'][:split_point]\n",
    "    train_dict['utterance'] = input_dict['utterance'][split_point:]\n",
    "    return train_dict, valid_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(path):\n",
    "    data = pd.read_csv(path)\n",
    "    input_dict = {\"utterance\":[]}\n",
    "    for _, row in tqdm(data.iterrows()):\n",
    "        for q_col in ['질문_1', '질문_2']:\n",
    "            for a_col in ['답변_1', '답변_2', '답변_3', '답변_4', '답변_5']:\n",
    "                input_dict[\"utterance\"].append({\"question\":row[q_col],\"answer\":row[a_col]})\n",
    "            \n",
    "    data = pd.read_csv('./bigdata/augs_preprocessing/qa_aug.csv')\n",
    "    for _, row in tqdm(data.iterrows()):\n",
    "        input_dict[\"utterance\"].append({\"question\":row['Question'],\"answer\":row['Answer']})\n",
    "    \n",
    "    \n",
    "    train_dict,valid_dict = split_input_dict(input_dict,ratio=0)\n",
    "    train_question = []\n",
    "    train_answer = []\n",
    "    for pairs in tqdm(train_dict['utterance']):\n",
    "        train_question.append(pairs['question'])\n",
    "        train_answer.append(pairs['answer'])\n",
    "\n",
    "    valid_question = []\n",
    "    valid_answer = []\n",
    "    for pairs in tqdm(valid_dict['utterance']):\n",
    "        valid_question.append(pairs['question'])\n",
    "        valid_answer.append(pairs['answer'])\n",
    "\n",
    "    \n",
    "    return train_question ,train_answer , valid_question , valid_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bbe732888d43eda980da1bff4adda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6068c05bb7c9443e84be2c7157d49cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f04d1ff8d7a4ab4aea9173180deb149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d3f09755bb42f59dc483caab920daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_q ,train_a , valid_q , valid_a = read_input(\"./bigdata/nobanmal/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "풍수지리를 활용하여 집을 꾸밀 때 어떤 점을 주의해야 할까요? ## 풍수지리를 고려할 때는 문의 위치, 가구 배치, 그리고 색상 선택에 신중하게 접근하세요.\n",
      "벽면에 사용되는 도장재료 중에서 페인트와 타일 중 어떤 것이 더 인기가 있나요? ## 페인트와 타일은 각자의 장단점에 의해 선택되는 경우가 많습니다. 페인트는 다양한 색상과 마무리 효과를 제공하며 경제적이고 사용이 간편한 재료로 평가받습니다. 반면에 타일은 특히 주방이나 화장실과 같이 습기가 많은 장소에 많이 사용되며 청결하고 튼튼한 특성으로 인기를 얻고 있습니다. 요즘에는 인기를 끌고 있는 재료 중에서는 특히 페인트의 다양한 색상과 효과, 그리고 타일의 내구성과 청결함이 더 많이 고려되고 있습니다.\n",
      "줄퍼티 마감은 무엇을 의미하나요? ## 줄퍼티 마감은 석고보드의 이음메 부분을 메꾼 후 1차, 2차로 퍼티를 해 마감하는 것을 의미합니다. 줄퍼티 마감은 올퍼티 마감보다 단가가 저렴하다는 장점이 있지만 이음메 부분의 배부름 현상이 있으며, 줄퍼티를 한 곳과 하지 않은 곳의 질감 차이가 있다는 단점이 있나요 주로 가게, 식당, 공장 등에서 시공됩니다.\n",
      "내 작은 주방을 활용한 인테리어 아이디어가 필요해요. ## 작은 주방을 활용하기 위해서는 벽 거치대와 다용도 수납을 활용할 수 있습니다. 또한, 주방을 밝은 컬러 스키마로 꾸미는 것도 좋은 아이디어입니다. 추가적으로, 테이블이 부족하다면 접이식 테이블을 활용하거나, 이동식 카트를 추가하여 작은 공간을 활용할 수 있습니다. 또한 거울을 활용하여 공간을 확장시키는 효과도 낼 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "####데이터 3개씩 끊어보는 기능\n",
    "n=2\n",
    "for ind,i in enumerate(zip(train_q,train_a)):\n",
    "    if ind>3:\n",
    "        break\n",
    "    print(i[0]+\" ## \"+i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####데이터 3개씩 끊어보는 기능\n",
    "n=2\n",
    "for ind,i in enumerate(zip(valid_q,valid_a)):\n",
    "    if ind>3:\n",
    "        break\n",
    "    print(i[0]+\" ## \"+i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedDataset:\n",
    "    def __init__(self, src_tokenizer, tgt_tokenizer,file_question , file_answer):\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = tgt_tokenizer\n",
    "        self.data_q = file_question\n",
    "        self.data_a = file_answer\n",
    "        self.data = [[self.data_q[i],self.data_a[i]] for i in range(len(self.data_q)) ]\n",
    "    def __getitem__(self, index):\n",
    "        src, trg = self.data_q[index] , self.data_a[index]\n",
    "        embeddings = self.src_tokenizer(src, return_attention_mask=False, return_token_type_ids=False)\n",
    "        embeddings['labels'] = self.trg_tokenizer(trg, return_attention_mask=False)['input_ids']\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PairedDataset(src_tokenizer, trg_tokenizer, train_q , train_a)\n",
    "eval_dataset = PairedDataset(src_tokenizer, trg_tokenizer, valid_q , valid_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['transformer.h.3.crossattention.q_attn.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.9.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.4.ln_cross_attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    'monologg/kobert',\n",
    "    'skt/kogpt2-base-v2',\n",
    "    pad_token_id=trg_tokenizer.bos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.config.decoder_start_token_id = trg_tokenizer.bos_token_id\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForSeq2Seq(src_tokenizer, model)\n",
    "\n",
    "arguments = Seq2SeqTrainingArguments(\n",
    "    output_dir='dacon-v0',\n",
    "    do_train=True,\n",
    "    #do_eval=True,\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    #eval_steps=24000,\n",
    "    logging_steps=6000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=6000,\n",
    "    num_train_epochs=800,\n",
    "    per_device_train_batch_size=16,\n",
    "    #per_device_eval_batch_size=16,\n",
    "    warmup_ratio=0.1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_total_limit=5,\n",
    "    dataloader_num_workers=1,\n",
    "    fp16=True,\n",
    "    #load_best_model_at_end=True,\n",
    "    #push_to_hub=True,\n",
    "    learning_rate = 5e-5\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    arguments,\n",
    "    data_collator=collator,\n",
    "    train_dataset=dataset,\n",
    "    #eval_dataset=eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1104000' max='1104000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1104000/1104000 57:40:07, Epoch 2400/2400]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.704400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>0.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192000</td>\n",
       "      <td>0.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240000</td>\n",
       "      <td>0.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288000</td>\n",
       "      <td>0.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336000</td>\n",
       "      <td>0.125100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384000</td>\n",
       "      <td>0.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432000</td>\n",
       "      <td>0.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480000</td>\n",
       "      <td>0.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528000</td>\n",
       "      <td>0.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576000</td>\n",
       "      <td>0.113100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624000</td>\n",
       "      <td>0.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672000</td>\n",
       "      <td>0.109600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720000</td>\n",
       "      <td>0.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768000</td>\n",
       "      <td>0.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816000</td>\n",
       "      <td>0.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864000</td>\n",
       "      <td>0.104900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>912000</td>\n",
       "      <td>0.104100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960000</td>\n",
       "      <td>0.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008000</td>\n",
       "      <td>0.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1056000</td>\n",
       "      <td>0.101900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1104000</td>\n",
       "      <td>0.101300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/leadawon5/gitfiles/venvs/myvenv/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1104000, training_loss=0.13692797763105752, metrics={'train_runtime': 207607.7714, 'train_samples_per_second': 85.003, 'train_steps_per_second': 5.318, 'total_flos': 5.878479789797299e+17, 'train_loss': 0.13692797763105752, 'epoch': 2400.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "#model.save_pretrained(\"checkpoints/dacon-v0/best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
