{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"MickyMike/graphcodebert-c\"\\n# tokenizer와 model은 미리 정의되어 있어야 합니다.\\n# device는 \\'cuda\\' 또는 \\'cpu\\'일 수 있습니다.\\n\\ndef predict(model, tokenizer, test_data, device, threshold=0.5):\\n    model.eval()  # 모델을 평가 모드로 설정\\n    predictions = []\\n    \\n    with torch.no_grad():  # 그래디언트 계산 비활성화\\n        for index, row in tqdm(test_data.iterrows(), total=test_data.shape[0]):\\n            # 코드 쌍을 토큰화합니다.\\n            inputs1 = tokenizer(row[\\'code1\\'], return_tensors=\\'pt\\', max_length=512, padding=\\'max_length\\', truncation=True).to(device)\\n            inputs2 = tokenizer(row[\\'code2\\'], return_tensors=\\'pt\\', max_length=512, padding=\\'max_length\\', truncation=True).to(device)\\n            \\n            # 모델을 통해 유사도 점수(로짓)를 계산합니다.\\n            logits = model(inputs1[\\'input_ids\\'], inputs1[\\'attention_mask\\'], inputs2[\\'input_ids\\'], inputs2[\\'attention_mask\\'])\\n            \\n            # 로짓을 확률로 변환하기 위해 sigmoid 함수를 적용합니다.\\n            probs = torch.sigmoid(logits).cpu().numpy()\\n            \\n            # 설정한 임계값을 기준으로 유사 여부를 판단합니다.\\n            prediction = 1 if probs > threshold else 0\\n            predictions.append(prediction)\\n    \\n    return predictions\\n\\n# 예제 사용\\ntest_data = pd.read_csv(\"./bigdata/test.csv\")\\n# 모델과 tokenizer가 정의되어 있어야 합니다.\\npredictions = predict(model, tokenizer, test_data, device, threshold=0.5)\\n\\n# 결과를 제출 파일로 저장\\nsubmission = pd.read_csv(\\'./bigdata/sample_submission.csv\\')\\nsubmission[\\'similar\\'] = predictions\\nsubmission.to_csv(\\'./bigdata/predictions_submit.csv\\', index=False)\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\"MickyMike/graphcodebert-c\"\n",
    "# tokenizer와 model은 미리 정의되어 있어야 합니다.\n",
    "# device는 'cuda' 또는 'cpu'일 수 있습니다.\n",
    "\n",
    "def predict(model, tokenizer, test_data, device, threshold=0.5):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for index, row in tqdm(test_data.iterrows(), total=test_data.shape[0]):\n",
    "            # 코드 쌍을 토큰화합니다.\n",
    "            text1 = remove_comments(row['code1'])\n",
    "            text2 = remove_comments(row['code2'])\n",
    "            inputs1 = tokenizer(text1, return_tensors='pt', max_length=512, padding='max_length', truncation=True).to(device)\n",
    "            inputs2 = tokenizer(text2, return_tensors='pt', max_length=512, padding='max_length', truncation=True).to(device)\n",
    "            \n",
    "            # 모델을 통해 유사도 점수(로짓)를 계산합니다.\n",
    "            logits = model(inputs1['input_ids'], inputs1['attention_mask'], inputs2['input_ids'], inputs2['attention_mask'])\n",
    "            \n",
    "            # 로짓을 확률로 변환하기 위해 sigmoid 함수를 적용합니다.\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            \n",
    "            # 설정한 임계값을 기준으로 유사 여부를 판단합니다.\n",
    "            prediction = 1 if probs > threshold else 0\n",
    "            predictions.append(prediction)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 예제 사용\n",
    "test_data = pd.read_csv(\"./bigdata/test.csv\")\n",
    "# 모델과 tokenizer가 정의되어 있어야 합니다.\n",
    "predictions = predict(model, tokenizer, test_data, device, threshold=0.5)\n",
    "\n",
    "# 결과를 제출 파일로 저장\n",
    "submission = pd.read_csv('./bigdata/sample_submission.csv')\n",
    "submission['similar'] = predictions\n",
    "submission.to_csv('./bigdata/predictions_submit.csv', index=False)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"neulab/codebert-cpp\"\n",
    "MODEL_TAG = \"neulab_codebert-cpp\"\n",
    "root_dir = \"/home/leadawon5/decs_jupyter_lab/gitfiles/DACON/code_similarity/bigdata/train_code\" \n",
    "\n",
    "class CodeEncoder(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME):\n",
    "        super(CodeEncoder, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output  # Use the pooled output\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodePairsDataset(Dataset):\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = []\n",
    "\n",
    "    def create_csv_dataset(self, root_dir, csv_file_path):\n",
    "        with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "            DIVIDER = 8\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"file_path1\", \"file_path2\", \"label\"])  # CSV 헤더\n",
    "\n",
    "            problem_dirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "            problem_files_count = {d: len([f for f in os.listdir(os.path.join(root_dir, d)) if f.endswith('.cpp')]) for d in problem_dirs}\n",
    "\n",
    "            # 긍정적 쌍 생성\n",
    "            # 긍정적 쌍 생성 과정 수정\n",
    "            for problem_dir, file_count in tqdm(problem_files_count.items(), desc=\"Creating positive pairs\"):\n",
    "                positive_pairs = []  # 긍정적 쌍을 저장할 임시 리스트\n",
    "                for i in range(1, file_count + 1):\n",
    "                    for j in range(i + 1, file_count + 1):\n",
    "                        file1 = os.path.join(root_dir, problem_dir, f\"{problem_dir}_{i}.cpp\")\n",
    "                        file2 = os.path.join(root_dir, problem_dir, f\"{problem_dir}_{j}.cpp\")\n",
    "                        positive_pairs.append([file1, file2, 1])  # 임시 리스트에 긍정적 쌍 추가\n",
    "\n",
    "                # 생성된 긍정적 쌍 중 절반을 랜덤하게 선택\n",
    "                num_pairs_to_keep = len(positive_pairs) // DIVIDER  # 유지할 긍정적 쌍의 수\n",
    "                selected_pairs = random.sample(positive_pairs, num_pairs_to_keep)  # 랜덤하게 절반 선택\n",
    "\n",
    "                # 선택된 긍정적 쌍을 CSV 파일에 쓰기\n",
    "                for pair in selected_pairs:\n",
    "                    writer.writerow(pair)  # CSV에 쓰기\n",
    "\n",
    "            # 부정적 쌍 생성\n",
    "            for problem_dir, file_count in tqdm(problem_files_count.items(), desc=\"Creating negative pairs\"):\n",
    "                num_neg_samples_per_file = file_count // (DIVIDER *2)\n",
    "\n",
    "                for i in range(1, file_count + 1):\n",
    "                    current_file = f\"{problem_dir}_{i}.cpp\"\n",
    "                    neg_samples_added = 0\n",
    "                    while neg_samples_added < num_neg_samples_per_file:\n",
    "                        other_dir = random.choice(list(problem_files_count.keys()))\n",
    "                        if other_dir == problem_dir:\n",
    "                            continue\n",
    "                        \n",
    "                        other_file_index = random.randint(1, problem_files_count[other_dir])\n",
    "                        other_file = f\"{other_dir}_{other_file_index}.cpp\"\n",
    "\n",
    "                        writer.writerow([os.path.join(root_dir, problem_dir, current_file), os.path.join(root_dir, other_dir, other_file), 0])\n",
    "                        neg_samples_added += 1\n",
    "\n",
    "    def load_from_csv(self, csv_file_path):\n",
    "        with open(csv_file_path, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # 헤더 건너뛰기\n",
    "            self.samples = [(row[0], row[1], int(row[2])) for row in reader]\n",
    "            \n",
    "\n",
    "\n",
    "    def _remove_comments(self,cpp_code):\n",
    "        # 멀티라인 주석 제거\n",
    "        code = re.sub(r'/\\*.*?\\*/', '', cpp_code, flags=re.DOTALL)\n",
    "        # 단일 라인 주석 제거\n",
    "        code = re.sub(r'//.*', '', code)\n",
    "        \n",
    "        # 문자열 내용 제거 (\" \" 안의 내용과 ' ' 안의 내용)\n",
    "        code = re.sub(r'\"(.*?)\"', '\"\"', code)\n",
    "        code = re.sub(r\"'(.*?)'\", \"''\", code)\n",
    "        # 빈 줄 제거\n",
    "        code = re.sub(r'\\n\\s*\\n', '\\n', code)\n",
    "        # 불필요한 공백 및 탭 변환 (연속된 공백을 하나의 공백으로)\n",
    "        code = re.sub(r'\\s+', ' ', code)\n",
    "        # 문자열 앞뒤 공백 제거\n",
    "        cleaned_code = code.strip()\n",
    "        \n",
    "        return cleaned_code\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path1, file_path2, label = self.samples[idx]\n",
    "        with open(file_path1, 'r', encoding='utf-8') as f:\n",
    "            text1 = self._remove_comments(f.read())\n",
    "        with open(file_path2, 'r', encoding='utf-8') as f:\n",
    "            text2 = self._remove_comments(f.read())\n",
    "        \n",
    "        inputs1 = self.tokenizer(text1, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        inputs2 = self.tokenizer(text2, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "\n",
    "        return {\n",
    "            \"input_ids1\": inputs1['input_ids'].squeeze(0),\n",
    "            \"attention_mask1\": inputs1['attention_mask'].squeeze(0),\n",
    "            \"input_ids2\": inputs2['input_ids'].squeeze(0),\n",
    "            \"attention_mask2\": inputs2['attention_mask'].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeComparisonModel(nn.Module):\n",
    "    def __init__(self, encoder_model_name):\n",
    "        super(CodeComparisonModel, self).__init__()\n",
    "        self.encoder = CodeEncoder(encoder_model_name)\n",
    "        # 두 임베딩을 결합한 후 사용할 추가적인 레이어를 정의합니다.\n",
    "        self.fc = nn.Linear(self.encoder.encoder.config.hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        embedding1 = self.encoder(input_ids1, attention_mask1)\n",
    "        embedding2 = self.encoder(input_ids2, attention_mask2)\n",
    "        # 두 임베딩을 결합합니다.\n",
    "        combined_embedding = torch.cat((embedding1, embedding2), 1)\n",
    "        # 결합된 임베딩을 추가적인 레이어에 통과시켜 이진 분류를 위한 로짓을 예측합니다.\n",
    "        logits = self.fc(combined_embedding)\n",
    "        # Sigmoid 함수를 적용하여 확률 값으로 변환\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        return probabilities.squeeze(-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, step, filepath, MODEL_TAG):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    checkpoint_path = os.path.join(filepath, f\"{MODEL_TAG}_checkpoint_epoch_{epoch}_step_{step}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "def train(model, data_loader, optimizer, device, start_epoch=0, start_step=0, epochs=2):\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for idx, batch in enumerate(data_loader, start=1):  # enumerate starts at 1 for correct modulo operation\n",
    "            if epoch == start_epoch and idx < start_step:\n",
    "                continue  # Skip to the saved step of the current epoch\n",
    "            optimizer.zero_grad()\n",
    "            input_ids1 = batch['input_ids1'].to(device)\n",
    "            attention_mask1 = batch['attention_mask1'].to(device)\n",
    "            input_ids2 = batch['input_ids2'].to(device)\n",
    "            attention_mask2 = batch['attention_mask2'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            criterion = nn.BCELoss()\n",
    "\n",
    "            outputs = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if idx % 6000 == 0:\n",
    "                save_checkpoint(model, optimizer, epoch, idx, \"./model/\", MODEL_TAG)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training loss: {total_loss / (len(data_loader)-start_step) if epoch == start_epoch else len(data_loader)}\")\n",
    "\n",
    "        # Adjust for next epochs\n",
    "        start_step = 0  # Reset start_step after the first resumed epoch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "dataset = CodePairsDataset(tokenizer)\n",
    "#dataset.create_csv_dataset(root_dir, f'./bigdata/csvs/{MODEL_TAG}_dataset.csv'\n",
    "                          \n",
    "                          )  # 데이터셋을 CSV 파일로 생성\n",
    "dataset.load_from_csv(f'./bigdata/csvs/{MODEL_TAG}_dataset.csv')  # 생성된 CSV 파일에서 데이터셋 로드\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_loader = DataLoader(dataset, batch_size=12, shuffle=True)\n",
    "model = CodeComparisonModel(MODEL_NAME).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, data_loader, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        checkpoint = torch.load(filepath)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        step = checkpoint.get('step', 0)  # Default to 0 if not found\n",
    "        print(f\"Checkpoint loaded from {filepath} at epoch {epoch}, step {step}\")\n",
    "        return epoch, step\n",
    "    else:\n",
    "        print(\"No checkpoint found at specified path!\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# 체크포인트 파일 경로 지정\n",
    "checkpoint_path = \"./bigdata/model/checkpoint_epoch_1.pth\"\n",
    "\n",
    "# 체크포인트 불러오기\n",
    "epoch, step = load_checkpoint(model, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if epoch is not None:\n",
    "#     train(model, data_loader, optimizer, device, start_epoch=epoch, start_step=step, epochs=desired_epochs, MODEL_TAG=MODEL_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_comments(cpp_code):\n",
    "        # 멀티라인 주석 제거\n",
    "        code = re.sub(r'/\\*.*?\\*/', '', cpp_code, flags=re.DOTALL)\n",
    "        # 단일 라인 주석 제거\n",
    "        code = re.sub(r'//.*', '', code)\n",
    "        \n",
    "        # 문자열 내용 제거 (\" \" 안의 내용과 ' ' 안의 내용)\n",
    "        code = re.sub(r'\"(.*?)\"', '\"\"', code)\n",
    "        code = re.sub(r\"'(.*?)'\", \"''\", code)\n",
    "        # 빈 줄 제거\n",
    "        code = re.sub(r'\\n\\s*\\n', '\\n', code)\n",
    "        # 불필요한 공백 및 탭 변환 (연속된 공백을 하나의 공백으로)\n",
    "        code = re.sub(r'\\s+', ' ', code)\n",
    "        # 문자열 앞뒤 공백 제거\n",
    "        cleaned_code = code.strip()\n",
    "        \n",
    "        return cleaned_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def infer(model, tokenizer, text1, text2, device, threshold=0.5):\n",
    "#     model.eval()  # 모델을 평가 모드로 설정\n",
    "#     with torch.no_grad():  # 그래디언트 계산을 비활성화\n",
    "#         # 두 코드 스니펫을 토큰화하고 디바이스로 이동\n",
    "#         text1 = remove_comments(text1)\n",
    "#         text2 = remove_comments(text2)\n",
    "#         inputs1 = tokenizer(text1, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
    "#         inputs2 = tokenizer(text2, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
    "        \n",
    "#         inputs1 = {k: v.to(device) for k, v in inputs1.items()}\n",
    "#         inputs2 = {k: v.to(device) for k, v in inputs2.items()}\n",
    "\n",
    "#         # 모델을 통해 유사도 점수(확률) 계산\n",
    "#         probabilities = model(**inputs1, **inputs2)\n",
    "\n",
    "#         # 유사도 점수를 기반으로 판단\n",
    "#         predicted_label = (probabilities > threshold).long()  # 확률이 임계값보다 크면 1, 아니면 0\n",
    "#         print(f\"Similarity score: {probabilities.item()}\")\n",
    "#         print(f\"Predicted label: {'Same' if predicted_label.item() == 1 else 'Different'}\")\n",
    "\n",
    "# # 추론 실행\n",
    "# infer(model, tokenizer, code_text1, code_text2, device, threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpvenv",
   "language": "python",
   "name": "vpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
