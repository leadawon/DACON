{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leadawon5/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"neulab/codebert-cpp\"\n",
    "MODEL_TAG = \"neulab_codebert-cpp\"\n",
    "root_dir = \"/home/leadawon5/decs_jupyter_lab/gitfiles/DACON/code_similarity/bigdata/train_code\" \n",
    "\n",
    "class CodeEncoder(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME):\n",
    "        super(CodeEncoder, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output  # Use the pooled output\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodePairsDataset(Dataset):\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = []\n",
    "\n",
    "    def create_csv_dataset(self, root_dir, csv_file_path):\n",
    "        with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "            DIVIDER = 2\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"file_path1\", \"file_path2\", \"label\"])  # CSV 헤더\n",
    "\n",
    "            problem_dirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "            problem_files_count = {d: len([f for f in os.listdir(os.path.join(root_dir, d)) if f.endswith('.cpp')]) for d in problem_dirs}\n",
    "\n",
    "            # 긍정적 쌍 생성\n",
    "            # 긍정적 쌍 생성 과정 수정\n",
    "            for problem_dir, file_count in tqdm(problem_files_count.items(), desc=\"Creating positive pairs\"):\n",
    "                positive_pairs = []  # 긍정적 쌍을 저장할 임시 리스트\n",
    "                for i in range(1, file_count + 1):\n",
    "                    for j in range(i + 1, file_count + 1):\n",
    "                        file1 = os.path.join(root_dir, problem_dir, f\"{problem_dir}_{i}.cpp\")\n",
    "                        file2 = os.path.join(root_dir, problem_dir, f\"{problem_dir}_{j}.cpp\")\n",
    "                        positive_pairs.append([file1, file2, 1])  # 임시 리스트에 긍정적 쌍 추가\n",
    "\n",
    "                # 생성된 긍정적 쌍 중 절반을 랜덤하게 선택\n",
    "                num_pairs_to_keep = len(positive_pairs) // DIVIDER  # 유지할 긍정적 쌍의 수\n",
    "                selected_pairs = random.sample(positive_pairs, num_pairs_to_keep)  # 랜덤하게 절반 선택\n",
    "\n",
    "                # 선택된 긍정적 쌍을 CSV 파일에 쓰기\n",
    "                for pair in selected_pairs:\n",
    "                    writer.writerow(pair)  # CSV에 쓰기\n",
    "\n",
    "            # 부정적 쌍 생성\n",
    "            for problem_dir, file_count in tqdm(problem_files_count.items(), desc=\"Creating negative pairs\"):\n",
    "                num_neg_samples_per_file = file_count // (DIVIDER *2)\n",
    "\n",
    "                for i in range(1, file_count + 1):\n",
    "                    current_file = f\"{problem_dir}_{i}.cpp\"\n",
    "                    neg_samples_added = 0\n",
    "                    while neg_samples_added < num_neg_samples_per_file:\n",
    "                        other_dir = random.choice(list(problem_files_count.keys()))\n",
    "                        if other_dir == problem_dir:\n",
    "                            continue\n",
    "                        \n",
    "                        other_file_index = random.randint(1, problem_files_count[other_dir])\n",
    "                        other_file = f\"{other_dir}_{other_file_index}.cpp\"\n",
    "\n",
    "                        writer.writerow([os.path.join(root_dir, problem_dir, current_file), os.path.join(root_dir, other_dir, other_file), 0])\n",
    "                        neg_samples_added += 1\n",
    "\n",
    "    def load_from_csv(self, csv_file_path):\n",
    "        with open(csv_file_path, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # 헤더 건너뛰기\n",
    "            self.samples = [(row[0], row[1], int(row[2])) for row in reader]\n",
    "            \n",
    "\n",
    "\n",
    "    def _remove_comments(self,cpp_code):\n",
    "        # 멀티라인 주석 제거\n",
    "        code = re.sub(r'/\\*.*?\\*/', '', cpp_code, flags=re.DOTALL)\n",
    "        # 단일 라인 주석 제거\n",
    "        code = re.sub(r'//.*', '', cleaned_code)\n",
    "        \n",
    "        # 문자열 내용 제거 (\" \" 안의 내용과 ' ' 안의 내용)\n",
    "        code = re.sub(r'\"(.*?)\"', '\"\"', code)\n",
    "        code = re.sub(r\"'(.*?)'\", \"''\", code)\n",
    "        # 빈 줄 제거\n",
    "        code = re.sub(r'\\n\\s*\\n', '\\n', code)\n",
    "        # 불필요한 공백 및 탭 변환 (연속된 공백을 하나의 공백으로)\n",
    "        code = re.sub(r'\\s+', ' ', code)\n",
    "        # 문자열 앞뒤 공백 제거\n",
    "        cleaned_code = code.strip()\n",
    "        \n",
    "        return cleaned_code\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path1, file_path2, label = self.samples[idx]\n",
    "        with open(file_path1, 'r', encoding='utf-8') as f:\n",
    "            text1 = self._remove_comments(f.read())\n",
    "        with open(file_path2, 'r', encoding='utf-8') as f:\n",
    "            text2 = self._remove_comments(f.read())\n",
    "        \n",
    "        inputs1 = self.tokenizer(text1, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        inputs2 = self.tokenizer(text2, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "\n",
    "        return {\n",
    "            \"input_ids1\": inputs1['input_ids'].squeeze(0),\n",
    "            \"attention_mask1\": inputs1['attention_mask'].squeeze(0),\n",
    "            \"input_ids2\": inputs2['input_ids'].squeeze(0),\n",
    "            \"attention_mask2\": inputs2['attention_mask'].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeComparisonModel(nn.Module):\n",
    "    def __init__(self, encoder_model_name):\n",
    "        super(CodeComparisonModel, self).__init__()\n",
    "        self.encoder = CodeEncoder(encoder_model_name)\n",
    "        # 두 임베딩을 결합한 후 사용할 추가적인 레이어를 정의합니다.\n",
    "        self.fc = nn.Linear(self.encoder.encoder.config.hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        embedding1 = self.encoder(input_ids1, attention_mask1)\n",
    "        embedding2 = self.encoder(input_ids2, attention_mask2)\n",
    "        # 두 임베딩을 결합합니다.\n",
    "        combined_embedding = torch.cat((embedding1, embedding2), 1)\n",
    "        # 결합된 임베딩을 추가적인 레이어에 통과시켜 이진 분류를 위한 로짓을 예측합니다.\n",
    "        logits = self.fc(combined_embedding)\n",
    "        # Sigmoid 함수를 적용하여 확률 값으로 변환\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        return probabilities.squeeze(-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch,step, filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    checkpoint_path = os.path.join(filepath, f\"{MODEL_TAG}_checkpoint_epoch_{epoch}_step_{step}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "def train(model, data_loader, optimizer, device, epochs=1):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for idx,batch in tqdm(enumerate(data_loader),desc=f\"{epoch} epoch is running!\"):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids1 = batch['input_ids1'].to(device)\n",
    "            attention_mask1 = batch['attention_mask1'].to(device)\n",
    "            input_ids2 = batch['input_ids2'].to(device)\n",
    "            attention_mask2 = batch['attention_mask2'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # BCELoss 인스턴스를 생성\n",
    "            criterion = nn.BCELoss()\n",
    "\n",
    "            # 모델의 forward pass를 실행\n",
    "            outputs = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "\n",
    "            # loss 계산\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if idx % 6000 == 0:\n",
    "                save_checkpoint(model, optimizer, epoch,idx, \"./model/\")\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training loss: {total_loss / len(data_loader)}\")\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "dataset = CodePairsDataset(tokenizer)\n",
    "#dataset.create_csv_dataset(root_dir, f'./bigdata/csvs/{MODEL_TAG}_dataset.csv')  # 데이터셋을 CSV 파일로 생성\n",
    "dataset.load_from_csv(f'./bigdata/csvs/{MODEL_TAG}_dataset.csv')  # 생성된 CSV 파일에서 데이터셋 로드\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neulab/codebert-cpp were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at neulab/codebert-cpp and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "model = CodeComparisonModel(MODEL_NAME).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 epoch is running!: 1it [00:16, 16.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./model/neulab_codebert-cpp_checkpoint_epoch_0_step_0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 epoch is running!: 6001it [1:35:52,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./model/neulab_codebert-cpp_checkpoint_epoch_0_step_6000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 epoch is running!: 12001it [2:54:38,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./model/neulab_codebert-cpp_checkpoint_epoch_0_step_12000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 epoch is running!: 18001it [4:01:59,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./model/neulab_codebert-cpp_checkpoint_epoch_0_step_18000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 epoch is running!: 24001it [5:01:01,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./model/neulab_codebert-cpp_checkpoint_epoch_0_step_24000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 epoch is running!: 30001it [5:55:19,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./model/neulab_codebert-cpp_checkpoint_epoch_0_step_30000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 epoch is running!: 36001it [6:46:51,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./model/neulab_codebert-cpp_checkpoint_epoch_0_step_36000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 epoch is running!: 42001it [7:38:04,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./model/neulab_codebert-cpp_checkpoint_epoch_0_step_42000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 epoch is running!: 48001it [8:26:48,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./model/neulab_codebert-cpp_checkpoint_epoch_0_step_48000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 epoch is running!: 54001it [9:14:54,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./model/neulab_codebert-cpp_checkpoint_epoch_0_step_54000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 epoch is running!: 54023it [9:15:20,  1.16s/it]"
     ]
    }
   ],
   "source": [
    "train(model, data_loader, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        checkpoint = torch.load(filepath)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded from {filepath} at epoch {epoch}\")\n",
    "        return epoch\n",
    "    else:\n",
    "        print(\"No checkpoint found at specified path!\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 체크포인트 파일 경로 지정\n",
    "checkpoint_path = \"./bigdata/model/checkpoint_epoch_1.pth\"\n",
    "\n",
    "# 체크포인트 불러오기\n",
    "epoch = load_checkpoint(model, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, tokenizer, text1, text2, device, threshold=0.5):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    with torch.no_grad():  # 그래디언트 계산을 비활성화\n",
    "        # 두 코드 스니펫을 토큰화하고 디바이스로 이동\n",
    "        inputs1 = tokenizer(text1, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
    "        inputs2 = tokenizer(text2, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
    "        \n",
    "        inputs1 = {k: v.to(device) for k, v in inputs1.items()}\n",
    "        inputs2 = {k: v.to(device) for k, v in inputs2.items()}\n",
    "\n",
    "        # 모델을 통해 유사도 점수(확률) 계산\n",
    "        probabilities = model(**inputs1, **inputs2)\n",
    "\n",
    "        # 유사도 점수를 기반으로 판단\n",
    "        predicted_label = (probabilities > threshold).long()  # 확률이 임계값보다 크면 1, 아니면 0\n",
    "        print(f\"Similarity score: {probabilities.item()}\")\n",
    "        print(f\"Predicted label: {'Same' if predicted_label.item() == 1 else 'Different'}\")\n",
    "\n",
    "# 추론 실행\n",
    "infer(model, tokenizer, code_text1, code_text2, device, threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpvenv",
   "language": "python",
   "name": "vpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
