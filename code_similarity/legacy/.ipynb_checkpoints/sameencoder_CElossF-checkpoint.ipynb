{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "root_dir = \"/home/leadawon5/decs_jupyter_lab/gitfiles/DACON/code_similarity/bigdata/train_code\" \n",
    "\n",
    "class CodeEncoder(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME):\n",
    "        super(CodeEncoder, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output  # Use the pooled output\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodePairsDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer, max_length=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = self._prepare_samples(root_dir)\n",
    "        \n",
    "#     def _prepare_samples(self, root_dir):\n",
    "#         samples = []\n",
    "#         problem_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "\n",
    "#         total_positive_pairs = 0\n",
    "#         total_files = 0\n",
    "\n",
    "#         # 긍정적 쌍의 총 개수와 파일의 총 개수 계산\n",
    "#         for problem_dir in tqdm(problem_dirs):\n",
    "#             cpp_files = [os.path.join(problem_dir, f) for f in os.listdir(problem_dir) if f.endswith('.cpp')]\n",
    "#             n = len(cpp_files)\n",
    "#             total_positive_pairs += n * (n - 1) // 2\n",
    "#             total_files += n\n",
    "\n",
    "#         # 각 파일 당 생성할 부정적 쌍의 개수\n",
    "#         avg_negative_pairs_per_file = total_positive_pairs // total_files if total_files else 0\n",
    "\n",
    "#         for problem_dir in tqdm(problem_dirs):\n",
    "#             cpp_files = [os.path.join(problem_dir, f) for f in os.listdir(problem_dir) if f.endswith('.cpp')]\n",
    "#             # 긍정적 쌍 추가\n",
    "#             for i in range(len(cpp_files)):\n",
    "#                 for j in range(i + 1, len(cpp_files)):\n",
    "#                     samples.append((cpp_files[i], cpp_files[j], 1))  # Positive pair\n",
    "\n",
    "#             # 부정적 쌍 추가\n",
    "#             other_problem_dirs = [d for d in problem_dirs if d != problem_dir]\n",
    "#             for file in cpp_files:\n",
    "#                 negative_samples = []\n",
    "#                 for other_dir in other_problem_dirs:\n",
    "#                     other_cpp_files = [os.path.join(other_dir, f) for f in os.listdir(other_dir) if f.endswith('.cpp')]\n",
    "#                     negative_samples.extend([(file, other_file, 0) for other_file in other_cpp_files])\n",
    "\n",
    "#                 # 부정적 쌍에서 랜덤으로 avg_negative_pairs_per_file 개수만큼 선택\n",
    "#                 if len(negative_samples) > avg_negative_pairs_per_file:\n",
    "#                     negative_samples = random.sample(negative_samples, avg_negative_pairs_per_file)\n",
    "#                 samples.extend(negative_samples)\n",
    "\n",
    "#         return samples\n",
    "    def _prepare_samples(self, root_dir):\n",
    "            samples = []\n",
    "            problem_dirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "            problem_files_count = {}\n",
    "\n",
    "            # 각 problemXXX 폴더의 파일 개수 계산\n",
    "            for problem_dir in problem_dirs:\n",
    "                problem_path = os.path.join(root_dir, problem_dir)\n",
    "                file_count = len([name for name in os.listdir(problem_path) if name.endswith('.cpp')])\n",
    "                problem_files_count[problem_dir] = file_count\n",
    "\n",
    "            # 긍정적 쌍 생성\n",
    "            for problem_dir, file_count in problem_files_count.items():\n",
    "                for i in range(1, file_count + 1):\n",
    "                    for j in range(i + 1, file_count + 1):\n",
    "                        file1 = f\"{problem_dir}_{i}.cpp\"\n",
    "                        file2 = f\"{problem_dir}_{j}.cpp\"\n",
    "                        samples.append((os.path.join(root_dir, problem_dir, file1), os.path.join(root_dir, problem_dir, file2), 1))\n",
    "\n",
    "            # 부정적 쌍 생성\n",
    "            for problem_dir, file_count in problem_files_count.items():\n",
    "                other_dirs = [d for d in problem_dirs if d != problem_dir]\n",
    "                for i in range(1, file_count + 1):\n",
    "                    file = f\"{problem_dir}_{i}.cpp\"\n",
    "                    negative_samples = []\n",
    "                    for other_dir in other_dirs:\n",
    "                        other_file_count = problem_files_count[other_dir]\n",
    "                        for k in range(1, other_file_count + 1):\n",
    "                            other_file = f\"{other_dir}_{k}.cpp\"\n",
    "                            negative_samples.append((os.path.join(root_dir, problem_dir, file), os.path.join(root_dir, other_dir, other_file), 0))\n",
    "\n",
    "                    # 부정적 쌍에서 랜덤으로 선택\n",
    "                    selected_negative_samples = random.sample(negative_samples, len(negative_samples))\n",
    "                    samples.extend(selected_negative_samples)\n",
    "\n",
    "            return samples\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path1, file_path2, label = self.samples[idx]\n",
    "        with open(file_path1, 'r', encoding='utf-8') as f:\n",
    "            text1 = f.read()\n",
    "        with open(file_path2, 'r', encoding='utf-8') as f:\n",
    "            text2 = f.read()\n",
    "\n",
    "        inputs1 = self.tokenizer(text1, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        inputs2 = self.tokenizer(text2, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "\n",
    "        return {\n",
    "            \"input_ids1\": inputs1['input_ids'].squeeze(0),\n",
    "            \"attention_mask1\": inputs1['attention_mask'].squeeze(0),\n",
    "            \"input_ids2\": inputs2['input_ids'].squeeze(0),\n",
    "            \"attention_mask2\": inputs2['attention_mask'].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeComparisonModel(nn.Module):\n",
    "    def __init__(self, encoder_model_name=MODEL_NAME):\n",
    "        super(CodeComparisonModel, self).__init__()\n",
    "        self.encoder = CodeEncoder(encoder_model_name)\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        embedding1 = self.encoder(input_ids1, attention_mask1)\n",
    "        embedding2 = self.encoder(input_ids2, attention_mask2)\n",
    "        similarity_scores = torch.matmul(embedding1, embedding2.T)\n",
    "        return similarity_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    checkpoint_path = os.path.join(filepath, f\"./bigdata/model/{MODEL_NAME}_checkpoint_epoch_{epoch}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "def train(model, data_loader, optimizer, device, epochs=1):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids1 = batch['input_ids1'].to(device)\n",
    "            attention_mask1 = batch['attention_mask1'].to(device)\n",
    "            input_ids2 = batch['input_ids2'].to(device)\n",
    "            attention_mask2 = batch['attention_mask2'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "            loss = F.cross_entropy(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training loss: {total_loss / len(data_loader)}\")\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        save_checkpoint(model, optimizer, epoch+1, \"./model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing dataset:   0%|                                | 0/500 [01:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4050571/2583361380.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCodePairsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCodeComparisonModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4050571/1976216457.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, tokenizer, max_length)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     def _prepare_samples(self, root_dir):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4050571/1976216457.py\u001b[0m in \u001b[0;36m_prepare_samples\u001b[0;34m(self, root_dir)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mother_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mother_problem_dirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0mother_cpp_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.cpp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                     \u001b[0mnegative_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mother_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mother_cpp_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;31m# 부정적 쌍에서 랜덤으로 avg_negative_pairs_per_file 개수만큼 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4050571/1976216457.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mother_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mother_problem_dirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0mother_cpp_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.cpp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                     \u001b[0mnegative_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mother_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mother_cpp_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;31m# 부정적 쌍에서 랜덤으로 avg_negative_pairs_per_file 개수만큼 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "dataset = CodePairsDataset(root_dir, tokenizer)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "model = CodeComparisonModel(MODEL_NAME).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(dataset, file)\n",
    "\n",
    "# 데이터셋 생성\n",
    "# dataset = CodePairsDataset(root_dir, tokenizer)  # 예시로 데이터셋을 생성하는 코드\n",
    "# 여기서 생성된 데이터셋을 pickle로 저장합니다.\n",
    "save_dataset(dataset, f'./bigdata/pickles/{MODEL_NAME}_dataset.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        dataset = pickle.load(file)\n",
    "    return dataset\n",
    "\n",
    "# 저장된 pickle 파일에서 데이터셋을 불러옵니다.\n",
    "dataset = load_dataset(f'./bigdata/pickles/{MODEL_NAME}_dataset.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, data_loader, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        checkpoint = torch.load(filepath)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded from {filepath} at epoch {epoch}\")\n",
    "        return epoch\n",
    "    else:\n",
    "        print(\"No checkpoint found at specified path!\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 체크포인트 파일 경로 지정\n",
    "checkpoint_path = \"./bigdata/model/checkpoint_epoch_1.pth\"\n",
    "\n",
    "# 체크포인트 불러오기\n",
    "epoch = load_checkpoint(model, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, tokenizer, text1, text2, device):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    with torch.no_grad():  # 그래디언트 계산을 비활성화\n",
    "        # 두 코드 스니펫을 토큰화\n",
    "        inputs1 = tokenizer(text1, return_tensors='pt', max_length=1024, padding='max_length', truncation=True).to(device)\n",
    "        inputs2 = tokenizer(text2, return_tensors='pt', max_length=1024, padding='max_length', truncation=True).to(device)\n",
    "        \n",
    "        # 모델을 통해 유사도 점수 계산\n",
    "        similarity_scores = model(inputs1['input_ids'], inputs1['attention_mask'], inputs2['input_ids'], inputs2['attention_mask'])\n",
    "        # 유사도 점수를 기반으로 판단 (여기서는 단순히 점수를 출력하고 있음)\n",
    "        print(\"Similarity score:\", similarity_scores.item())\n",
    "\n",
    "# 두 코드 스니펫의 예시 텍스트\n",
    "code_text1 = \"def sum(a, b): return a + b\"\n",
    "code_text2 = \"def add(x, y): return x + y\"\n",
    "\n",
    "# 추론 실행\n",
    "infer(model, tokenizer, code_text1, code_text2, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpvenv",
   "language": "python",
   "name": "vpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
