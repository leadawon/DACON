{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"neulab/codebert-cpp\"\n",
    "\n",
    "# 모델 정의\n",
    "class CodeBERTDPR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CodeBERTDPR, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output  # [CLS] 토큰의 출력 사용\n",
    "\n",
    "# # 데이터셋 정의\n",
    "# class CodePairsDataset(Dataset):\n",
    "#     def __init__(self, tokenizer, code_pairs, labels):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.code_pairs = code_pairs\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.code_pairs)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         code_pair = self.code_pairs[idx]\n",
    "#         label = self.labels[idx]\n",
    "\n",
    "#         # 토큰화 및 인코딩\n",
    "#         encoding = self.tokenizer(code_pair[0], code_pair[1], return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "#         input_ids = encoding['input_ids'].squeeze(0)\n",
    "#         attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "#         return input_ids, attention_mask, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodePairsDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = []\n",
    "        self.root_dir = root_dir\n",
    "        self._prepare_dataset()\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        problem_dirs = [os.path.join(self.root_dir, d) for d in os.listdir(self.root_dir) if os.path.isdir(os.path.join(self.root_dir, d))]\n",
    "        problem_files = {}\n",
    "\n",
    "        for dir in problem_dirs:\n",
    "            problem_id = os.path.basename(dir)\n",
    "            cpp_files = [os.path.join(dir, \"cpp\", f) for f in os.listdir(os.path.join(dir, \"cpp\")) if f.endswith('.cpp')]\n",
    "            problem_files[problem_id] = cpp_files\n",
    "\n",
    "        for problem_id, files in problem_files.items():\n",
    "            # 긍정적인 샘플 추가\n",
    "            for i in range(len(files)):\n",
    "                for j in range(i + 1, len(files)):\n",
    "                    self.samples.append((files[i], files[j], 1))  # 긍정적인 쌍\n",
    "\n",
    "            # 부정적인 샘플 추가\n",
    "            other_problems = list(set(problem_files.keys()) - {problem_id})\n",
    "            for file in files:\n",
    "                other_problem_id = random.choice(other_problems)\n",
    "                other_file = random.choice(problem_files[other_problem_id])\n",
    "                self.samples.append((file, other_file, 0))  # 부정적인 쌍\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file1, file2, label = self.samples[idx]\n",
    "\n",
    "        with open(file1, 'r', encoding='utf-8') as f:\n",
    "            text1 = f.read()\n",
    "        \n",
    "        with open(file2, 'r', encoding='utf-8') as f:\n",
    "            text2 = f.read()\n",
    "\n",
    "        # 토큰화 및 인코딩\n",
    "        encoding = self.tokenizer(text1, text2, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 준비 (가정된 예시 데이터)\n",
    "code_pairs_example = [\n",
    "    (\"def sum(a, b): return a + b\", \"def add(x, y): return x + y\"),  # 긍정적 예시\n",
    "    (\"def sum(a, b): return a + b\", \"def subtract(x, y): return x - y\")  # 부정적 예시\n",
    "]\n",
    "labels_example = [1, 0]  # 1: 긍정적, 0: 부정적\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/leadawon5/decs_jupyter_lab/gitfiles/DACON/code_similarity/bigdata/train_code\" \n",
    "dataset = CodePairsDataset(root_dir, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# dataset = CodePairsDataset(tokenizer, code_pairs_example, labels_example)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# 모델 학습\n",
    "model = CodeBERTDPR().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):  # 예시에서는 1 에포크로 제한\n",
    "    for input_ids, attention_mask, labels in data_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 모델을 통해 각 코드 스니펫 인코딩\n",
    "        embeddings = model(input_ids, attention_mask)\n",
    "\n",
    "        # 유사도 계산\n",
    "        similarities = torch.matmul(embeddings, embeddings.T)\n",
    "\n",
    "        # 손실 계산 (여기서는 간단화를 위해 실제 DPR 손실과 다를 수 있음)\n",
    "        loss = F.cross_entropy(similarities, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    # 두 개의 코드 스니펫을 준비합니다 (예시로 첫 번째와 두 번째 스니펫을 사용)\n",
    "    input_ids_1, attention_mask_1, _ = dataset[0]  # 첫 번째 예시\n",
    "    input_ids_2, attention_mask_2, _ = dataset[1]  # 두 번째 예시\n",
    "    \n",
    "    # 각 코드 스니펫에 대해 임베딩을 계산\n",
    "    input_ids_1 = input_ids_1.unsqueeze(0).to(device)  # 배치 차원 추가\n",
    "    attention_mask_1 = attention_mask_1.unsqueeze(0).to(device)\n",
    "    embeddings_1 = model(input_ids_1, attention_mask_1)\n",
    "    \n",
    "    input_ids_2 = input_ids_2.unsqueeze(0).to(device)\n",
    "    attention_mask_2 = attention_mask_2.unsqueeze(0).to(device)\n",
    "    embeddings_2 = model(input_ids_2, attention_mask_2)\n",
    "    \n",
    "    # 두 임베딩 간의 내적을 계산하여 유사도 점수를 얻음\n",
    "    similarity_score = torch.matmul(embeddings_1, embeddings_2.T)\n",
    "    \n",
    "    print(f\"Similarity Score: {similarity_score.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpvenv",
   "language": "python",
   "name": "vpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
