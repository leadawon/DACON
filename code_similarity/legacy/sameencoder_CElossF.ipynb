{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "MODEL_TAG = \"microsoft_codebert-base\"\n",
    "root_dir = \"/home/leadawon5/decs_jupyter_lab/gitfiles/DACON/code_similarity/bigdata/train_code\" \n",
    "\n",
    "class CodeEncoder(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME):\n",
    "        super(CodeEncoder, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output  # Use the pooled output\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodePairsDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer, max_length=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = self._prepare_samples(root_dir)\n",
    "        \n",
    "#     def _prepare_samples(self, root_dir):\n",
    "#         samples = []\n",
    "#         problem_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "\n",
    "#         total_positive_pairs = 0\n",
    "#         total_files = 0\n",
    "\n",
    "#         # 긍정적 쌍의 총 개수와 파일의 총 개수 계산\n",
    "#         for problem_dir in tqdm(problem_dirs):\n",
    "#             cpp_files = [os.path.join(problem_dir, f) for f in os.listdir(problem_dir) if f.endswith('.cpp')]\n",
    "#             n = len(cpp_files)\n",
    "#             total_positive_pairs += n * (n - 1) // 2\n",
    "#             total_files += n\n",
    "\n",
    "#         # 각 파일 당 생성할 부정적 쌍의 개수\n",
    "#         avg_negative_pairs_per_file = total_positive_pairs // total_files if total_files else 0\n",
    "\n",
    "#         for problem_dir in tqdm(problem_dirs):\n",
    "#             cpp_files = [os.path.join(problem_dir, f) for f in os.listdir(problem_dir) if f.endswith('.cpp')]\n",
    "#             # 긍정적 쌍 추가\n",
    "#             for i in range(len(cpp_files)):\n",
    "#                 for j in range(i + 1, len(cpp_files)):\n",
    "#                     samples.append((cpp_files[i], cpp_files[j], 1))  # Positive pair\n",
    "\n",
    "#             # 부정적 쌍 추가\n",
    "#             other_problem_dirs = [d for d in problem_dirs if d != problem_dir]\n",
    "#             for file in cpp_files:\n",
    "#                 negative_samples = []\n",
    "#                 for other_dir in other_problem_dirs:\n",
    "#                     other_cpp_files = [os.path.join(other_dir, f) for f in os.listdir(other_dir) if f.endswith('.cpp')]\n",
    "#                     negative_samples.extend([(file, other_file, 0) for other_file in other_cpp_files])\n",
    "\n",
    "#                 # 부정적 쌍에서 랜덤으로 avg_negative_pairs_per_file 개수만큼 선택\n",
    "#                 if len(negative_samples) > avg_negative_pairs_per_file:\n",
    "#                     negative_samples = random.sample(negative_samples, avg_negative_pairs_per_file)\n",
    "#                 samples.extend(negative_samples)\n",
    "\n",
    "#         return samples\n",
    "\n",
    "\n",
    "    def _prepare_samples(self, root_dir):\n",
    "        samples = []\n",
    "        problem_dirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        problem_files_count = {d: len([f for f in os.listdir(os.path.join(root_dir, d)) if f.endswith('.cpp')]) for d in problem_dirs}\n",
    "\n",
    "        # 긍정적 쌍 생성\n",
    "        for problem_dir, file_count in tqdm(problem_files_count.items(),desc=\"pos set\"):\n",
    "            for i in range(1, file_count + 1):\n",
    "                for j in range(i + 1, file_count + 1):\n",
    "                    file1 = os.path.join(root_dir, problem_dir, f\"{problem_dir}_{i}.cpp\")\n",
    "                    file2 = os.path.join(root_dir, problem_dir, f\"{problem_dir}_{j}.cpp\")\n",
    "                    samples.append((file1, file2, 1))\n",
    "\n",
    "        # 부정적 쌍 생성\n",
    "        for problem_dir, file_count in tqdm(problem_files_count.items(), desc=\"neg set\"):\n",
    "            for i in range(1, file_count + 1):\n",
    "                # 현재 폴더의 파일\n",
    "                current_file = f\"{problem_dir}_{i}.cpp\"\n",
    "\n",
    "                # 가능한 모든 부정적 쌍의 개수를 계산\n",
    "                total_neg_samples = sum(problem_files_count.values()) - file_count\n",
    "\n",
    "                # 각 파일에 대해 생성할 부정적 쌍의 개수\n",
    "                num_neg_samples_per_file = total_neg_samples // len(problem_files_count)\n",
    "\n",
    "                # 다른 폴더의 파일과의 부정적 쌍 생성\n",
    "                neg_samples_added = 0\n",
    "                while neg_samples_added < num_neg_samples_per_file:\n",
    "                    # 다른 폴더를 랜덤하게 선택\n",
    "                    other_dir = random.choice(list(problem_files_count.keys()))\n",
    "                    if other_dir == problem_dir:\n",
    "                        continue\n",
    "\n",
    "                    # 다른 폴더의 파일 중 하나를 랜덤하게 선택\n",
    "                    other_file_index = random.randint(1, problem_files_count[other_dir])\n",
    "                    other_file = f\"{other_dir}_{other_file_index}.cpp\"\n",
    "\n",
    "                    samples.append((os.path.join(root_dir, problem_dir, current_file), os.path.join(root_dir, other_dir, other_file), 0))\n",
    "                    neg_samples_added += 1\n",
    "\n",
    "        return samples\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path1, file_path2, label = self.samples[idx]\n",
    "        with open(file_path1, 'r', encoding='utf-8') as f:\n",
    "            text1 = f.read()\n",
    "        with open(file_path2, 'r', encoding='utf-8') as f:\n",
    "            text2 = f.read()\n",
    "\n",
    "        inputs1 = self.tokenizer(text1, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        inputs2 = self.tokenizer(text2, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "\n",
    "        return {\n",
    "            \"input_ids1\": inputs1['input_ids'].squeeze(0),\n",
    "            \"attention_mask1\": inputs1['attention_mask'].squeeze(0),\n",
    "            \"input_ids2\": inputs2['input_ids'].squeeze(0),\n",
    "            \"attention_mask2\": inputs2['attention_mask'].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeComparisonModel(nn.Module):\n",
    "    def __init__(self, encoder_model_name=MODEL_NAME):\n",
    "        super(CodeComparisonModel, self).__init__()\n",
    "        self.encoder = CodeEncoder(encoder_model_name)\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        embedding1 = self.encoder(input_ids1, attention_mask1)\n",
    "        embedding2 = self.encoder(input_ids2, attention_mask2)\n",
    "        similarity_scores = torch.matmul(embedding1, embedding2.T)\n",
    "        return similarity_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    checkpoint_path = os.path.join(filepath, f\"./bigdata/model/{MODEL_TAG}_checkpoint_epoch_{epoch}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "def train(model, data_loader, optimizer, device, epochs=1):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids1 = batch['input_ids1'].to(device)\n",
    "            attention_mask1 = batch['attention_mask1'].to(device)\n",
    "            input_ids2 = batch['input_ids2'].to(device)\n",
    "            attention_mask2 = batch['attention_mask2'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "            loss = F.cross_entropy(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training loss: {total_loss / len(data_loader)}\")\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        save_checkpoint(model, optimizer, epoch+1, \"./model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos set:  61%|███████████████████▋            | 307/500 [02:15<01:42,  1.88it/s]"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "dataset = CodePairsDataset(root_dir, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(dataset, file)\n",
    "\n",
    "# 데이터셋 생성\n",
    "# dataset = CodePairsDataset(root_dir, tokenizer)  # 예시로 데이터셋을 생성하는 코드\n",
    "# 여기서 생성된 데이터셋을 pickle로 저장합니다.\n",
    "save_dataset(dataset, f'./bigdata/pickles/{MODEL_TAG}_dataset.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        dataset = pickle.load(file)\n",
    "    return dataset\n",
    "\n",
    "# 저장된 pickle 파일에서 데이터셋을 불러옵니다.\n",
    "dataset = load_dataset(f'./bigdata/pickles/{MODEL_TAG}_dataset.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "model = CodeComparisonModel(MODEL_NAME).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, data_loader, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        checkpoint = torch.load(filepath)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded from {filepath} at epoch {epoch}\")\n",
    "        return epoch\n",
    "    else:\n",
    "        print(\"No checkpoint found at specified path!\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 체크포인트 파일 경로 지정\n",
    "checkpoint_path = \"./bigdata/model/checkpoint_epoch_1.pth\"\n",
    "\n",
    "# 체크포인트 불러오기\n",
    "epoch = load_checkpoint(model, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, tokenizer, text1, text2, device):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    with torch.no_grad():  # 그래디언트 계산을 비활성화\n",
    "        # 두 코드 스니펫을 토큰화\n",
    "        inputs1 = tokenizer(text1, return_tensors='pt', max_length=1024, padding='max_length', truncation=True).to(device)\n",
    "        inputs2 = tokenizer(text2, return_tensors='pt', max_length=1024, padding='max_length', truncation=True).to(device)\n",
    "        \n",
    "        # 모델을 통해 유사도 점수 계산\n",
    "        similarity_scores = model(inputs1['input_ids'], inputs1['attention_mask'], inputs2['input_ids'], inputs2['attention_mask'])\n",
    "        # 유사도 점수를 기반으로 판단 (여기서는 단순히 점수를 출력하고 있음)\n",
    "        print(\"Similarity score:\", similarity_scores.item())\n",
    "\n",
    "# 두 코드 스니펫의 예시 텍스트\n",
    "code_text1 = \"def sum(a, b): return a + b\"\n",
    "code_text2 = \"def add(x, y): return x + y\"\n",
    "\n",
    "# 추론 실행\n",
    "infer(model, tokenizer, code_text1, code_text2, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpvenv",
   "language": "python",
   "name": "vpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
