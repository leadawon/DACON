{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"MickyMike/graphcodebert-c\"\\n# tokenizer와 model은 미리 정의되어 있어야 합니다.\\n# device는 \\'cuda\\' 또는 \\'cpu\\'일 수 있습니다.\\n\\ndef predict(model, tokenizer, test_data, device, threshold=0.5):\\n    model.eval()  # 모델을 평가 모드로 설정\\n    predictions = []\\n    \\n    with torch.no_grad():  # 그래디언트 계산 비활성화\\n        for index, row in tqdm(test_data.iterrows(), total=test_data.shape[0]):\\n            # 코드 쌍을 토큰화합니다.\\n            text1 = remove_comments(row[\\'code1\\'])\\n            text2 = remove_comments(row[\\'code2\\'])\\n            inputs1 = tokenizer(text1, return_tensors=\\'pt\\', max_length=512, padding=\\'max_length\\', truncation=True).to(device)\\n            inputs2 = tokenizer(text2, return_tensors=\\'pt\\', max_length=512, padding=\\'max_length\\', truncation=True).to(device)\\n            \\n            # 모델을 통해 유사도 점수(로짓)를 계산합니다.\\n            logits = model(inputs1[\\'input_ids\\'], inputs1[\\'attention_mask\\'], inputs2[\\'input_ids\\'], inputs2[\\'attention_mask\\'])\\n            \\n            # 로짓을 확률로 변환하기 위해 sigmoid 함수를 적용합니다.\\n            probs = torch.sigmoid(logits).cpu().numpy()\\n            \\n            # 설정한 임계값을 기준으로 유사 여부를 판단합니다.\\n            prediction = 1 if probs > threshold else 0\\n            predictions.append(prediction)\\n    \\n    return predictions\\n\\n# 예제 사용\\ntest_data = pd.read_csv(\"./bigdata/test.csv\")\\n# 모델과 tokenizer가 정의되어 있어야 합니다.\\npredictions = predict(model, tokenizer, test_data, device, threshold=0.5)\\n\\n# 결과를 제출 파일로 저장\\nsubmission = pd.read_csv(\\'./bigdata/sample_submission.csv\\')\\nsubmission[\\'similar\\'] = predictions\\nsubmission.to_csv(\\'./bigdata/predictions_submit.csv\\', index=False)\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\"MickyMike/graphcodebert-c\"\n",
    "# tokenizer와 model은 미리 정의되어 있어야 합니다.\n",
    "# device는 'cuda' 또는 'cpu'일 수 있습니다.\n",
    "\n",
    "def predict(model, tokenizer, test_data, device, threshold=0.5):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for index, row in tqdm(test_data.iterrows(), total=test_data.shape[0]):\n",
    "            # 코드 쌍을 토큰화합니다.\n",
    "            text1 = remove_comments(row['code1'])\n",
    "            text2 = remove_comments(row['code2'])\n",
    "            inputs1 = tokenizer(text1, return_tensors='pt', max_length=512, padding='max_length', truncation=True).to(device)\n",
    "            inputs2 = tokenizer(text2, return_tensors='pt', max_length=512, padding='max_length', truncation=True).to(device)\n",
    "            \n",
    "            # 모델을 통해 유사도 점수(로짓)를 계산합니다.\n",
    "            logits = model(inputs1['input_ids'], inputs1['attention_mask'], inputs2['input_ids'], inputs2['attention_mask'])\n",
    "            \n",
    "            # 로짓을 확률로 변환하기 위해 sigmoid 함수를 적용합니다.\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            \n",
    "            # 설정한 임계값을 기준으로 유사 여부를 판단합니다.\n",
    "            prediction = 1 if probs > threshold else 0\n",
    "            predictions.append(prediction)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 예제 사용\n",
    "test_data = pd.read_csv(\"./bigdata/test.csv\")\n",
    "# 모델과 tokenizer가 정의되어 있어야 합니다.\n",
    "predictions = predict(model, tokenizer, test_data, device, threshold=0.5)\n",
    "\n",
    "# 결과를 제출 파일로 저장\n",
    "submission = pd.read_csv('./bigdata/sample_submission.csv')\n",
    "submission['similar'] = predictions\n",
    "submission.to_csv('./bigdata/predictions_submit.csv', index=False)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leadawon5/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"MickyMike/graphcodebert-c\"\n",
    "MODEL_TAG = \"MickyMike_graphcodebert-c\"\n",
    "root_dir = \"/home/leadawon5/decs_jupyter_lab/gitfiles/DACON/code_similarity/bigdata/train_code\" \n",
    "\n",
    "class CodeEncoder(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME):\n",
    "        super(CodeEncoder, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output  # Use the pooled output\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodePairsDataset(Dataset):\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = []\n",
    "\n",
    "    def create_csv_dataset(self, root_dir, csv_file_path):\n",
    "        with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "            DIVIDER = 8\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"file_path1\", \"file_path2\", \"label\"])  # CSV 헤더\n",
    "\n",
    "            problem_dirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "            problem_files_count = {d: len([f for f in os.listdir(os.path.join(root_dir, d)) if f.endswith('.cpp')]) for d in problem_dirs}\n",
    "\n",
    "            # 긍정적 쌍 생성\n",
    "            # 긍정적 쌍 생성 과정 수정\n",
    "            for problem_dir, file_count in tqdm(problem_files_count.items(), desc=\"Creating positive pairs\"):\n",
    "                positive_pairs = []  # 긍정적 쌍을 저장할 임시 리스트\n",
    "                for i in range(1, file_count + 1):\n",
    "                    for j in range(i + 1, file_count + 1):\n",
    "                        file1 = os.path.join(root_dir, problem_dir, f\"{problem_dir}_{i}.cpp\")\n",
    "                        file2 = os.path.join(root_dir, problem_dir, f\"{problem_dir}_{j}.cpp\")\n",
    "                        positive_pairs.append([file1, file2, 1])  # 임시 리스트에 긍정적 쌍 추가\n",
    "\n",
    "                # 생성된 긍정적 쌍 중 절반을 랜덤하게 선택\n",
    "                num_pairs_to_keep = len(positive_pairs) // DIVIDER  # 유지할 긍정적 쌍의 수\n",
    "                selected_pairs = random.sample(positive_pairs, num_pairs_to_keep)  # 랜덤하게 절반 선택\n",
    "\n",
    "                # 선택된 긍정적 쌍을 CSV 파일에 쓰기\n",
    "                for pair in selected_pairs:\n",
    "                    writer.writerow(pair)  # CSV에 쓰기\n",
    "\n",
    "            # 부정적 쌍 생성\n",
    "            for problem_dir, file_count in tqdm(problem_files_count.items(), desc=\"Creating negative pairs\"):\n",
    "                num_neg_samples_per_file = file_count // (DIVIDER *2)\n",
    "\n",
    "                for i in range(1, file_count + 1):\n",
    "                    current_file = f\"{problem_dir}_{i}.cpp\"\n",
    "                    neg_samples_added = 0\n",
    "                    while neg_samples_added < num_neg_samples_per_file:\n",
    "                        other_dir = random.choice(list(problem_files_count.keys()))\n",
    "                        if other_dir == problem_dir:\n",
    "                            continue\n",
    "                        \n",
    "                        other_file_index = random.randint(1, problem_files_count[other_dir])\n",
    "                        other_file = f\"{other_dir}_{other_file_index}.cpp\"\n",
    "\n",
    "                        writer.writerow([os.path.join(root_dir, problem_dir, current_file), os.path.join(root_dir, other_dir, other_file), 0])\n",
    "                        neg_samples_added += 1\n",
    "\n",
    "    def load_from_csv(self, csv_file_path):\n",
    "        with open(csv_file_path, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # 헤더 건너뛰기\n",
    "            self.samples = [(row[0], row[1], int(row[2])) for row in reader]\n",
    "            \n",
    "\n",
    "\n",
    "    def _remove_comments(self,cpp_code):\n",
    "        # 멀티라인 주석 제거\n",
    "        code = re.sub(r'/\\*.*?\\*/', '', cpp_code, flags=re.DOTALL)\n",
    "        # 단일 라인 주석 제거\n",
    "        code = re.sub(r'//.*', '', code)\n",
    "        \n",
    "        # 문자열 내용 제거 (\" \" 안의 내용과 ' ' 안의 내용)\n",
    "        code = re.sub(r'\"(.*?)\"', '\"\"', code)\n",
    "        code = re.sub(r\"'(.*?)'\", \"''\", code)\n",
    "        # 빈 줄 제거\n",
    "        code = re.sub(r'\\n\\s*\\n', '\\n', code)\n",
    "        # 불필요한 공백 및 탭 변환 (연속된 공백을 하나의 공백으로)\n",
    "        code = re.sub(r'\\s+', ' ', code)\n",
    "        # 문자열 앞뒤 공백 제거\n",
    "        cleaned_code = code.strip()\n",
    "        \n",
    "        return cleaned_code\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path1, file_path2, label = self.samples[idx]\n",
    "        with open(file_path1, 'r', encoding='utf-8') as f:\n",
    "            text1 = self._remove_comments(f.read())\n",
    "        with open(file_path2, 'r', encoding='utf-8') as f:\n",
    "            text2 = self._remove_comments(f.read())\n",
    "        \n",
    "        inputs1 = self.tokenizer(text1, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        inputs2 = self.tokenizer(text2, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "\n",
    "        return {\n",
    "            \"input_ids1\": inputs1['input_ids'].squeeze(0),\n",
    "            \"attention_mask1\": inputs1['attention_mask'].squeeze(0),\n",
    "            \"input_ids2\": inputs2['input_ids'].squeeze(0),\n",
    "            \"attention_mask2\": inputs2['attention_mask'].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeComparisonModel(nn.Module):\n",
    "    def __init__(self, encoder_model_name):\n",
    "        super(CodeComparisonModel, self).__init__()\n",
    "        self.encoder = CodeEncoder(encoder_model_name)\n",
    "        # 두 임베딩을 결합한 후 사용할 추가적인 레이어를 정의합니다.\n",
    "        self.fc = nn.Linear(self.encoder.encoder.config.hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        embedding1 = self.encoder(input_ids1, attention_mask1)\n",
    "        embedding2 = self.encoder(input_ids2, attention_mask2)\n",
    "        # 두 임베딩을 결합합니다.\n",
    "        combined_embedding = torch.cat((embedding1, embedding2), 1)\n",
    "        # 결합된 임베딩을 추가적인 레이어에 통과시켜 이진 분류를 위한 로짓을 예측합니다.\n",
    "        logits = self.fc(combined_embedding)\n",
    "        # Sigmoid 함수를 적용하여 확률 값으로 변환\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        return probabilities.squeeze(-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, step, filepath, MODEL_TAG):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    checkpoint_path = os.path.join(filepath, f\"{MODEL_TAG}_checkpoint_epoch_{epoch}_step_{step}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "def train(model, data_loader, optimizer, device, start_epoch=0, start_step=0, epochs=2):\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for idx, batch in enumerate(data_loader, start=1):  # enumerate starts at 1 for correct modulo operation\n",
    "            if epoch == start_epoch and idx < start_step:\n",
    "                continue  # Skip to the saved step of the current epoch\n",
    "            optimizer.zero_grad()\n",
    "            input_ids1 = batch['input_ids1'].to(device)\n",
    "            attention_mask1 = batch['attention_mask1'].to(device)\n",
    "            input_ids2 = batch['input_ids2'].to(device)\n",
    "            attention_mask2 = batch['attention_mask2'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            criterion = nn.BCELoss()\n",
    "\n",
    "            outputs = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if idx % 6000 == 0:\n",
    "                save_checkpoint(model, optimizer, epoch, idx, \"./model/\", MODEL_TAG)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training loss: {total_loss / (len(data_loader)-start_step) if epoch == start_epoch else len(data_loader)}\")\n",
    "\n",
    "        # Adjust for next epochs\n",
    "        start_step = 0  # Reset start_step after the first resumed epoch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|████| 1.30k/1.30k [00:00<00:00, 116kB/s]\n",
      "Downloading vocab.json: 100%|████████████████| 798k/798k [00:00<00:00, 1.07MB/s]\n",
      "Downloading merges.txt: 100%|████████████████| 456k/456k [00:00<00:00, 41.9MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 1.46MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████| 772/772 [00:00<00:00, 346kB/s]\n",
      "Creating positive pairs: 100%|████████████████| 500/500 [05:01<00:00,  1.66it/s]\n",
      "Creating negative pairs: 100%|████████████████| 500/500 [01:59<00:00,  4.17it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "dataset = CodePairsDataset(tokenizer)\n",
    "dataset.create_csv_dataset(root_dir, f'./bigdata/csvs/{MODEL_TAG}_dataset.csv'\n",
    "                          \n",
    "                          )  # 데이터셋을 CSV 파일로 생성\n",
    "dataset.load_from_csv(f'./bigdata/csvs/{MODEL_TAG}_dataset.csv')  # 생성된 CSV 파일에서 데이터셋 로드\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|█████████████████| 748/748 [00:00<00:00, 86.6kB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████| 499M/499M [00:29<00:00, 17.2MB/s]\n",
      "Some weights of the model checkpoint at MickyMike/graphcodebert-c were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at MickyMike/graphcodebert-c and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_loader = DataLoader(dataset, batch_size=240, shuffle=True)\n",
    "model = CodeComparisonModel(MODEL_NAME).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_6000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_12000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_18000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_24000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_30000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_36000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_42000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_48000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_54000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_60000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_66000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_72000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_78000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_84000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_90000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_96000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_102000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_108000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_114000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_120000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_126000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_132000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_138000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_144000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_150000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_156000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_162000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_168000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_174000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_180000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_186000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_192000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_198000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_204000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_210000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_216000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_222000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_228000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_234000.pth\n",
      "Checkpoint saved to ./model/MickyMike_graphcodebert-c_checkpoint_epoch_0_step_240000.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4068147/3362209404.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4068147/4112111236.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer, device, start_epoch, start_step, epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4068147/3601852511.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids1, attention_mask1, input_ids2, attention_mask2)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0membedding1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0membedding2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# 두 임베딩을 결합합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcombined_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4068147/1897123832.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m  \u001b[0;31m# Use the pooled output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m         )\n\u001b[1;32m    864\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    532\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m                 )\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m         )\n\u001b[1;32m    456\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitfiles/venvs/vpvenv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, data_loader, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        checkpoint = torch.load(filepath)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        step = checkpoint.get('step', 0)  # Default to 0 if not found\n",
    "        print(f\"Checkpoint loaded from {filepath} at epoch {epoch}, step {step}\")\n",
    "        return epoch, step\n",
    "    else:\n",
    "        print(\"No checkpoint found at specified path!\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# 체크포인트 파일 경로 지정\n",
    "checkpoint_path = \"./bigdata/model/checkpoint_epoch_1.pth\"\n",
    "\n",
    "# 체크포인트 불러오기\n",
    "epoch, step = load_checkpoint(model, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if epoch is not None:\n",
    "#     train(model, data_loader, optimizer, device, start_epoch=epoch, start_step=step, epochs=desired_epochs, MODEL_TAG=MODEL_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_comments(cpp_code):\n",
    "        # 멀티라인 주석 제거\n",
    "        code = re.sub(r'/\\*.*?\\*/', '', cpp_code, flags=re.DOTALL)\n",
    "        # 단일 라인 주석 제거\n",
    "        code = re.sub(r'//.*', '', code)\n",
    "        \n",
    "        # 문자열 내용 제거 (\" \" 안의 내용과 ' ' 안의 내용)\n",
    "        code = re.sub(r'\"(.*?)\"', '\"\"', code)\n",
    "        code = re.sub(r\"'(.*?)'\", \"''\", code)\n",
    "        # 빈 줄 제거\n",
    "        code = re.sub(r'\\n\\s*\\n', '\\n', code)\n",
    "        # 불필요한 공백 및 탭 변환 (연속된 공백을 하나의 공백으로)\n",
    "        code = re.sub(r'\\s+', ' ', code)\n",
    "        # 문자열 앞뒤 공백 제거\n",
    "        cleaned_code = code.strip()\n",
    "        \n",
    "        return cleaned_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def infer(model, tokenizer, text1, text2, device, threshold=0.5):\n",
    "#     model.eval()  # 모델을 평가 모드로 설정\n",
    "#     with torch.no_grad():  # 그래디언트 계산을 비활성화\n",
    "#         # 두 코드 스니펫을 토큰화하고 디바이스로 이동\n",
    "#         text1 = remove_comments(text1)\n",
    "#         text2 = remove_comments(text2)\n",
    "#         inputs1 = tokenizer(text1, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
    "#         inputs2 = tokenizer(text2, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
    "        \n",
    "#         inputs1 = {k: v.to(device) for k, v in inputs1.items()}\n",
    "#         inputs2 = {k: v.to(device) for k, v in inputs2.items()}\n",
    "\n",
    "#         # 모델을 통해 유사도 점수(확률) 계산\n",
    "#         probabilities = model(**inputs1, **inputs2)\n",
    "\n",
    "#         # 유사도 점수를 기반으로 판단\n",
    "#         predicted_label = (probabilities > threshold).long()  # 확률이 임계값보다 크면 1, 아니면 0\n",
    "#         print(f\"Similarity score: {probabilities.item()}\")\n",
    "#         print(f\"Predicted label: {'Same' if predicted_label.item() == 1 else 'Different'}\")\n",
    "\n",
    "# # 추론 실행\n",
    "# infer(model, tokenizer, code_text1, code_text2, device, threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpvenv",
   "language": "python",
   "name": "vpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
