{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train / Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>code1</th>\n",
       "      <th>code2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000000</td>\n",
       "      <td>#include &lt;bits/stdc++.h&gt;\\nusing namespace std;...</td>\n",
       "      <td>#include &lt;bits/stdc++.h&gt;\\n#define rep(i, n) fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_000001</td>\n",
       "      <td>#include&lt;bits/stdc++.h&gt;\\n#define rep(i,n)for(i...</td>\n",
       "      <td>// //bitset操作\\n// #include &lt;iostream&gt;\\n// #inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_000002</td>\n",
       "      <td>#include &lt;bits/stdc++.h&gt;\\nusing namespace std;...</td>\n",
       "      <td>#include &lt;bits/stdc++.h&gt;\\n#include &lt;ext/pb_ds/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_000003</td>\n",
       "      <td>#include &lt;bits/stdc++.h&gt;\\nusing namespace std;...</td>\n",
       "      <td>#include &lt;bits/stdc++.h&gt;\\nusing namespace std;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_000004</td>\n",
       "      <td>#include&lt;bits/stdc++.h&gt;\\nusing namespace std;\\...</td>\n",
       "      <td>#include&lt;iostream&gt;\\n#include&lt;algorithm&gt;\\n#incl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pair_id                                              code1  \\\n",
       "0  TEST_000000  #include <bits/stdc++.h>\\nusing namespace std;...   \n",
       "1  TEST_000001  #include<bits/stdc++.h>\\n#define rep(i,n)for(i...   \n",
       "2  TEST_000002  #include <bits/stdc++.h>\\nusing namespace std;...   \n",
       "3  TEST_000003  #include <bits/stdc++.h>\\nusing namespace std;...   \n",
       "4  TEST_000004  #include<bits/stdc++.h>\\nusing namespace std;\\...   \n",
       "\n",
       "                                               code2  \n",
       "0  #include <bits/stdc++.h>\\n#define rep(i, n) fo...  \n",
       "1  // //bitset操作\\n// #include <iostream>\\n// #inc...  \n",
       "2  #include <bits/stdc++.h>\\n#include <ext/pb_ds/...  \n",
       "3  #include <bits/stdc++.h>\\nusing namespace std;...  \n",
       "4  #include<iostream>\\n#include<algorithm>\\n#incl...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"./bigdata/test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_comments(cpp_code):\n",
    "        # 멀티라인 주석 제거\n",
    "        code = re.sub(r'/\\*.*?\\*/', '', cpp_code, flags=re.DOTALL)\n",
    "        # 단일 라인 주석 제거\n",
    "        code = re.sub(r'//.*', '', code)\n",
    "        \n",
    "        # 문자열 내용 제거 (\" \" 안의 내용과 ' ' 안의 내용)\n",
    "        code = re.sub(r'\"(.*?)\"', '\"\"', code)\n",
    "        code = re.sub(r\"'(.*?)'\", \"''\", code)\n",
    "        # 빈 줄 제거\n",
    "        code = re.sub(r'\\n\\s*\\n', '\\n', code)\n",
    "        # 불필요한 공백 및 탭 변환 (연속된 공백을 하나의 공백으로)\n",
    "        code = re.sub(r'\\s+', ' ', code)\n",
    "        # 문자열 앞뒤 공백 제거\n",
    "        cleaned_code = code.strip()\n",
    "        \n",
    "        return cleaned_code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model (CountVectorizer+CosineSimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel():\n",
    "    def __init__(self):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        \n",
    "        self.encoder = AutoModel.from_pretrained(\"neulab/codebert-cpp\").to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"neulab/codebert-cpp\")\n",
    "    \n",
    "    def predict_proba(self, code1, code2):\n",
    "        # 입력 받은 코드 쌍으로 부터 vectorizer를 통해 vector화 합니다.\n",
    "        inputs1 = self.tokenizer(remove_comments(code1), return_tensors='pt', max_length=512, padding='max_length', truncation=True).to(device)\n",
    "        inputs2 = self.tokenizer(remove_comments(code2), return_tensors='pt', max_length=512, padding='max_length', truncation=True).to(device)\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        # 코드 벡터를 생성합니다.\n",
    "        with torch.no_grad():\n",
    "            outputs1 = self.encoder(**inputs1)\n",
    "            outputs2 = self.encoder(**inputs2)\n",
    "\n",
    "        # pooler_output을 사용하여 코드 벡터를 가져옵니다.\n",
    "        code1_vec = outputs1.pooler_output\n",
    "        code2_vec = outputs2.pooler_output\n",
    "\n",
    "        # 벡터 간 유사도를 계산합니다.\n",
    "        similarity = cosine_similarity(code1_vec.cpu().numpy(), code2_vec.cpu().numpy())[0][0]\n",
    "\n",
    "        \n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    def predict(self, code1_series, code2_series):\n",
    "        result = []\n",
    "        for code1,code2 in tqdm(zip(code1_series,code2_series)):\n",
    "            preds = self.predict_proba(code1, code2)\n",
    "            result.append(preds)\n",
    "        # cosine-similarity (유사도)가 설정한 임계값(Threshold=0.5)보다 높다면 유사하다 : 1, 아니라면 유사하지 않다 : 0\n",
    "        \n",
    "        # 각 코드 쌍들의 유사도를 Threshold를 통해 유사함을 판별 (이진분류)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neulab/codebert-cpp were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at neulab/codebert-cpp and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "595000it [3:15:48, 50.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# 모델 추론\n",
    "model = BaselineModel()\n",
    "preds = model.predict(test['code1'], test['code2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.where(np.array(preds)>0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./bigdata/sample_submission.csv')\n",
    "submission['similar'] = preds\n",
    "submission.to_csv('./bigdata/base_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpvenv",
   "language": "python",
   "name": "vpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
